{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing Data"
      ],
      "metadata": {
        "id": "FclFxNpim9xq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q yfinance dateparser"
      ],
      "metadata": {
        "id": "gZJi-z4swdLk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jCDpMp4OwCw-"
      },
      "outputs": [],
      "source": [
        "# Yahoo Stock Price API package\n",
        "import yfinance\n",
        "\n",
        "# Data Manipulation packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime as dt\n",
        "import dateparser\n",
        "import re\n",
        "\n",
        "# Visualization\n",
        "import plotly.graph_objects as go\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# EDA & Preprocessor packages\n",
        "from sklearn.preprocessing import RobustScaler, MinMaxScaler\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "\n",
        "# ML model packages\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Evaluation Metrics packages\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "# For Logging Purpose\n",
        "# Connect to gsheets\n",
        "from google.colab import auth, files, drive\n",
        "import gspread\n",
        "from google.auth import default\n",
        "import os\n",
        "\n",
        "\n",
        "# Yahoo Stock Price API package\n",
        "import yfinance\n",
        "\n",
        "# Data Manipulation packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime as dt\n",
        "import dateparser\n",
        "import re\n",
        "\n",
        "# Processor\n",
        "from numba import njit\n",
        "\n",
        "# Disable pandas warning\n",
        "pd.options.mode.chained_assignment = None\n",
        "\n",
        "# Visualization\n",
        "import plotly.graph_objects as go\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# EDA & Preprocessor packages\n",
        "from sklearn.preprocessing import RobustScaler, MinMaxScaler\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "\n",
        "# DL model packages\n",
        "import tensorflow as tf\n",
        "\n",
        "# Evaluation Metrics packages\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "# For Logging Purpose\n",
        "# Connect to gsheets\n",
        "from google.colab import auth, files, drive\n",
        "import gspread\n",
        "from google.auth import default\n",
        "from uuid import uuid4\n",
        "import os\n",
        "\n",
        "# Yahoo Stock Price API package\n",
        "import yfinance\n",
        "\n",
        "# Data Manipulation packages\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numba import njit\n",
        "import math\n",
        "import dateparser\n",
        "import re\n",
        "\n",
        "import datetime as dt\n",
        "\n",
        "# Visualization\n",
        "import plotly.graph_objects as go\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# EDA & Preprocessor packages\n",
        "from sklearn.preprocessing import RobustScaler, MinMaxScaler\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "\n",
        "# Deeplearning Lib\n",
        "import tensorflow as tf\n",
        "\n",
        "# Evaluation Metrics packages\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "# For Logging Purpose\n",
        "# Connect to gsheets\n",
        "from google.colab import auth, files, drive\n",
        "import gspread\n",
        "from google.auth import default\n",
        "\n",
        "# Other\n",
        "from typing import Tuple\n",
        "from uuid import uuid4\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "id": "uoT4wvrrjS0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## DATASET CONFIGURATION ##\n",
        "gmt = 7 # GMT+7\n",
        "today = dt.datetime.now() + dt.timedelta(hours=gmt) \n",
        "parameters = {\n",
        "    \"stock_market_detail\": {\n",
        "        \"ticker\": \"APIC.JK\",\n",
        "        \"start_date\": \"1993-02-01\",\n",
        "        \"end_date\": \"2022-10-31\",\n",
        "        \"used_cols\": [\"Open\", \"High\", \"Low\", \"Close\"]\n",
        "    },\n",
        "    \"macro_economy_features\": {\n",
        "        \"bi_rate\": False,\n",
        "        \"inflasi\": True,\n",
        "        \"jisdor\": True,\n",
        "        \"m2\": True,\n",
        "        \"vix\": True\n",
        "    },\n",
        "    \"micro_economy_features\": {\n",
        "        # Choices = [APIC, BWPT, EDGE, HAIS, JRPT, MITI, PSKT, SCCO, SIDO, SMCB, TOWR, None]\n",
        "        \"ticker\": \"SIDO\" # Change to 'None' to exclude micro economy feature\n",
        "    },\n",
        "    \"preprocessing_hyperparameters\": {\n",
        "        \"window\": 24,\n",
        "        \"rolling_agg\": \"mean\",\n",
        "        \"fillna\": -99999 # will not be used if dropna == True\n",
        "    },\n",
        "    \"preprocessing_treatments\": {\n",
        "        \"rolling_window\": False,\n",
        "        \"dropna\": True\n",
        "    },\n",
        "    \"feature_extraction\": {\n",
        "        \"high_low_pct\": True,\n",
        "        \"pct_change\": True\n",
        "    },\n",
        "    \"data_preparation\": {\n",
        "        \"features\": [\"Open\", \"High\", \"Low\"],\n",
        "        \"target_prediction\": \"Close\",\n",
        "        \"test_size\": .3,\n",
        "        \"validation_size\": 0,\n",
        "        \"n_in\": 3\n",
        "    },\n",
        "    \"model_parameter\": {\n",
        "        \"selected_model\": \"linear_regression\",\n",
        "        \"model_options\": {\n",
        "            \"linear_regression\": {},\n",
        "            \"random_forest\": {\n",
        "                \"random_state\": 2022,\n",
        "                \"n_estimators\": 100,\n",
        "                \"criterion\": \"squared_error\", # [\"absolute_error\", \"poison\"]\n",
        "                \"max_depth\": None,\n",
        "                \"min_samples_split\": 2,\n",
        "                \"min_samples_leaf\": 1,\n",
        "                \"min_weight_fraction_leaf\": 0.0,\n",
        "                \"max_features\": 1, # [\"sqrt\", \"log\", None],\n",
        "                \"max_leaf_nodes\": None,\n",
        "                \"min_impurity_decrease\": 0,\n",
        "                \"bootstrap\": True,\n",
        "                \"oob_score\": False,\n",
        "                \"max_samples\": None\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "model_option = {\n",
        "    \"linear_regression\": LinearRegression,\n",
        "    \"random_forest\": RandomForestRegressor\n",
        "}"
      ],
      "metadata": {
        "id": "IT1gG1YDpT70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download stock market data\n",
        "stock_market = yfinance.download(\n",
        "    parameters[\"stock_market_detail\"][\"ticker\"],\n",
        "    start=parameters[\"stock_market_detail\"][\"start_date\"],\n",
        "    end=parameters[\"stock_market_detail\"][\"end_date\"]\n",
        ")\n",
        "\n",
        "# Select only necessary features\n",
        "stock_market = stock_market.loc[:, parameters[\"stock_market_detail\"][\"used_cols\"]]"
      ],
      "metadata": {
        "id": "C0mtwrjipPgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(stock_market)"
      ],
      "metadata": {
        "id": "N_whQUmTpWPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stock_market.info()"
      ],
      "metadata": {
        "id": "v8S34vzRUC1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get Features"
      ],
      "metadata": {
        "id": "Y4bTsS2aoW5-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Makro"
      ],
      "metadata": {
        "id": "ME6vcfDL2_KN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "makro_config = {\n",
        "    \"macro_economy_features\": {\n",
        "        \"bi_rate\": {\n",
        "            \"read_data_config\": {\n",
        "                \"usecols\": [\"Tanggal\", \"BI-7Day-RR\"]\n",
        "            },\n",
        "            \"cols_selector\": {\n",
        "                \"date\": \"Date\",\n",
        "                \"bi_rate\": \"BI-Rate\"\n",
        "            },\n",
        "            \"cols_rename\": [\"Date\", \"BI-Rate\"],\n",
        "            \"preprocessing\": {\n",
        "                \"merge\": \"left\",\n",
        "                \"fillna\": \"interpolate\" # After merge\n",
        "            }\n",
        "        },\n",
        "        \"inflasi\": {\n",
        "            \"read_data_config\": {\n",
        "                \"usecols\": [\"Periode\", \"Data Inflasi\"],\n",
        "                \"skiprows\": 4\n",
        "            },\n",
        "            \"cols_selector\": {\n",
        "                \"date\": \"Date\",\n",
        "                \"inflasi\": \"Inflasi\"\n",
        "            },\n",
        "            \"cols_rename\": [\"Date\", \"Inflasi\"],\n",
        "            \"preprocessing\": {\n",
        "                \"merge\": \"left\",\n",
        "                \"fillna\": \"interpolate\" # After merge\n",
        "            }\n",
        "        },\n",
        "        \"jisdor\": {\n",
        "            \"read_data_config\": {\n",
        "                \"usecols\": [\"Tanggal\", \"Kurs\"]\n",
        "            },\n",
        "            \"cols_selector\": {\n",
        "                \"date\": \"Date\",\n",
        "                \"kurs\": \"Kurs\"\n",
        "            },\n",
        "            \"cols_rename\": [\"Date\", \"Kurs\"],\n",
        "            \"preprocessing\": {\n",
        "                \"merge\": \"left\",\n",
        "                \"fillna\": \"interpolate\" # After merge\n",
        "            }\n",
        "        },\n",
        "        \"m2\": {\n",
        "            \"read_data_config\": {\n",
        "                \"usecols\": [\"Tahun\", \"Uang Beredar Luas (M2)\"]\n",
        "            },\n",
        "            \"cols_selector\": {\n",
        "                \"date\": \"Date\",\n",
        "                \"m2\": \"M2\"\n",
        "            },\n",
        "            \"cols_rename\": [\"Date\", \"M2\"],\n",
        "            \"preprocessing\": {\n",
        "                \"merge\": \"left\",\n",
        "                \"fillna\": \"interpolate\" # After merge\n",
        "            }\n",
        "        },\n",
        "        \"vix\": {\n",
        "            \"read_data_config\": {\n",
        "                \"usecols\": [\"Date\", \"Open\", \"High\", \"Low\", \"Close\"],\n",
        "                \"parse_dates\": [\"Date\"]\n",
        "            },\n",
        "            \"cols_selector\": {\n",
        "                \"date\": \"Date\",\n",
        "                \"value\": [\"Vix Open\", \"Vix High\", \"Vix Low\", \"Vix Close\"]\n",
        "            },\n",
        "            \"cols_rename\": [\"Date\", \"Vix Open\", \"Vix High\", \"Vix Low\", \"Vix Close\"],\n",
        "            \"preprocessing\": {\n",
        "                \"merge\": \"left\",\n",
        "                \"fillna\": \"interpolate\"\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "tk5pXmU3sAH4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp_stock_market = stock_market.reset_index()\n",
        "temp_stock_market[\"Date\"] = pd.to_datetime(temp_stock_market[\"Date\"], utc=True)\n",
        "temp_stock_market[\"Date\"] = temp_stock_market[\"Date\"].dt.date"
      ],
      "metadata": {
        "id": "riyBqF8axMtG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GDRIVE_PATH = \"drive/MyDrive\"\n",
        "FEATURES_DATASET_PATH = \"Datasets/Features\"\n",
        "MAKRO_EKONOMI_PATH = os.path.join(GDRIVE_PATH, FEATURES_DATASET_PATH, \"Macro Economi\")"
      ],
      "metadata": {
        "id": "J7pCBmJ5jdku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if parameters[\"macro_economy_features\"][\"bi_rate\"]:\n",
        "  # Unpack config\n",
        "  bi_rate_c = makro_config[\"macro_economy_features\"][\"bi_rate\"]\n",
        "\n",
        "  # Read Data\n",
        "  bi_rate = pd.read_excel(\n",
        "      os.path.join(MAKRO_EKONOMI_PATH, \"BI-7Day-RR.xlsx\"), \n",
        "      **bi_rate_c[\"read_data_config\"]\n",
        "      )\n",
        "\n",
        "  # Rename\n",
        "  bi_rate.columns = bi_rate_c[\"cols_rename\"]\n",
        "\n",
        "  # Fix date format\n",
        "  bi_rate[bi_rate_c[\"cols_selector\"][\"date\"]] = bi_rate[bi_rate_c[\"cols_selector\"][\"date\"]].apply(dateparser.parse)\n",
        "  bi_rate.sort_values(bi_rate_c[\"cols_selector\"][\"date\"], inplace=True)\n",
        "  bi_rate.reset_index(drop=True, inplace=True)\n",
        "\n",
        "  # Fix data format\n",
        "  bi_rate[bi_rate_c[\"cols_selector\"][\"bi_rate\"]] = bi_rate[bi_rate_c[\"cols_selector\"][\"bi_rate\"]].apply(lambda value: value.replace(\"%\", \"\"))\n",
        "  bi_rate[bi_rate_c[\"cols_selector\"][\"bi_rate\"]] = bi_rate[bi_rate_c[\"cols_selector\"][\"bi_rate\"]].apply(str.strip)\n",
        "  bi_rate[bi_rate_c[\"cols_selector\"][\"bi_rate\"]] = bi_rate[bi_rate_c[\"cols_selector\"][\"bi_rate\"]].astype(float)\n",
        "\n",
        "  # Merge\n",
        "  temp_stock_market = temp_stock_market.merge(bi_rate, how=bi_rate_c[\"preprocessing\"][\"merge\"], on=\"Date\")\n",
        "\n",
        "  if bi_rate_c[\"preprocessing\"].get(\"fillna\"):\n",
        "    temp_stock_market[bi_rate_c[\"cols_selector\"][\"bi_rate\"]] = temp_stock_market[bi_rate_c[\"cols_selector\"][\"bi_rate\"]].interpolate(method=\"linear\")\n",
        "\n",
        "  parameters[\"data_preparation\"][\"features\"] += [bi_rate_c[\"cols_selector\"][\"bi_rate\"]]\n",
        "  parameters[\"data_preparation\"][\"features\"] = list(set(parameters[\"data_preparation\"][\"features\"]))"
      ],
      "metadata": {
        "id": "65zMspscwcOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if parameters[\"macro_economy_features\"][\"inflasi\"]:\n",
        "  # Unpack config\n",
        "  inflasi_c = makro_config[\"macro_economy_features\"][\"inflasi\"]\n",
        "\n",
        "  # Read Data\n",
        "  inflasi = pd.read_excel(\n",
        "      os.path.join(MAKRO_EKONOMI_PATH, \"Data Inflasi.xlsx\"), \n",
        "      **inflasi_c[\"read_data_config\"]\n",
        "      )\n",
        "  \n",
        "  # Rename\n",
        "  inflasi.columns = inflasi_c[\"cols_rename\"]\n",
        "\n",
        "  # Fix date format\n",
        "  inflasi[inflasi_c[\"cols_selector\"][\"date\"]] = inflasi[inflasi_c[\"cols_selector\"][\"date\"]].apply(dateparser.parse)\n",
        "  inflasi.sort_values(inflasi_c[\"cols_selector\"][\"date\"], inplace=True)\n",
        "  inflasi.reset_index(drop=True, inplace=True)\n",
        "\n",
        "  # Fix data format\n",
        "  inflasi[inflasi_c[\"cols_selector\"][\"inflasi\"]] = inflasi[inflasi_c[\"cols_selector\"][\"inflasi\"]].apply(lambda value: value.replace(\"%\", \"\"))\n",
        "  inflasi[inflasi_c[\"cols_selector\"][\"inflasi\"]] = inflasi[inflasi_c[\"cols_selector\"][\"inflasi\"]].apply(str.strip)\n",
        "  inflasi[inflasi_c[\"cols_selector\"][\"inflasi\"]] = inflasi[inflasi_c[\"cols_selector\"][\"inflasi\"]].astype(float)\n",
        "  temp_stock_market.Date = temp_stock_market.Date.astype(\"datetime64[ns]\")\n",
        "  # inflasi.Date = inflasi.Date.astype(\"datetime64[ns]\")\n",
        "  temp_stock_market = temp_stock_market.merge(inflasi, how=inflasi_c[\"preprocessing\"][\"merge\"], on=\"Date\")\n",
        "\n",
        "  if inflasi_c[\"preprocessing\"].get(\"fillna\"):\n",
        "    temp_stock_market[inflasi_c[\"cols_selector\"][\"inflasi\"]] = temp_stock_market[inflasi_c[\"cols_selector\"][\"inflasi\"]].interpolate(method=\"linear\")\n",
        "\n",
        "  parameters[\"data_preparation\"][\"features\"] += [inflasi_c[\"cols_selector\"][\"inflasi\"]]\n",
        "  parameters[\"data_preparation\"][\"features\"] = list(set(parameters[\"data_preparation\"][\"features\"]))"
      ],
      "metadata": {
        "id": "XvSjL6QLvTCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if parameters[\"macro_economy_features\"][\"jisdor\"]:\n",
        "  # Unpack config\n",
        "  jisdor_c = makro_config[\"macro_economy_features\"][\"jisdor\"]\n",
        "\n",
        "  # Read Data\n",
        "  jisdor = pd.read_excel(\n",
        "      os.path.join(MAKRO_EKONOMI_PATH, \"Informasi Kurs Jisdor.xlsx\"), \n",
        "      **jisdor_c[\"read_data_config\"]\n",
        "      )\n",
        "  \n",
        "  # Rename\n",
        "  jisdor.columns = jisdor_c[\"cols_rename\"]\n",
        "\n",
        "  # Fix date format\n",
        "  jisdor[jisdor_c[\"cols_selector\"][\"date\"]] = jisdor[jisdor_c[\"cols_selector\"][\"date\"]].apply(dateparser.parse)\n",
        "  jisdor.sort_values(jisdor_c[\"cols_selector\"][\"date\"], inplace=True)\n",
        "  jisdor.reset_index(drop=True, inplace=True)\n",
        "\n",
        "  temp_stock_market = temp_stock_market.merge(jisdor, how=jisdor_c[\"preprocessing\"][\"merge\"], on=\"Date\")\n",
        "\n",
        "  if jisdor_c[\"preprocessing\"].get(\"fillna\"):\n",
        "    temp_stock_market[jisdor_c[\"cols_selector\"][\"kurs\"]] = temp_stock_market[jisdor_c[\"cols_selector\"][\"kurs\"]].interpolate(method=\"linear\")\n",
        "  \n",
        "  parameters[\"data_preparation\"][\"features\"] += [jisdor_c[\"cols_selector\"][\"kurs\"]]\n",
        "  parameters[\"data_preparation\"][\"features\"] = list(set(parameters[\"data_preparation\"][\"features\"]))"
      ],
      "metadata": {
        "id": "QSKV6nbLvO_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if parameters[\"macro_economy_features\"][\"m2\"]:\n",
        "  # Unpack config\n",
        "  m2_c = makro_config[\"macro_economy_features\"][\"m2\"]\n",
        "\n",
        "  # Read Data\n",
        "  m2 = pd.read_excel(\n",
        "      os.path.join(MAKRO_EKONOMI_PATH, \"M2.xlsx\"), \n",
        "      **m2_c[\"read_data_config\"]\n",
        "      )\n",
        "  \n",
        "  # Rename\n",
        "  m2.columns = m2_c[\"cols_rename\"]\n",
        "\n",
        "  # Fix data format\n",
        "  m2[m2_c[\"cols_selector\"][\"m2\"]] = m2[m2_c[\"cols_selector\"][\"m2\"]].apply(lambda value: value.replace(\",\", \"\"))\n",
        "  m2[m2_c[\"cols_selector\"][\"m2\"]] = m2[m2_c[\"cols_selector\"][\"m2\"]].apply(str.strip)\n",
        "  m2[m2_c[\"cols_selector\"][\"m2\"]] = m2[m2_c[\"cols_selector\"][\"m2\"]].astype(float)\n",
        "\n",
        "  temp_stock_market = temp_stock_market.merge(m2, how=m2_c[\"preprocessing\"][\"merge\"], on=\"Date\")\n",
        "\n",
        "  if m2_c[\"preprocessing\"].get(\"fillna\"):\n",
        "    temp_stock_market[m2_c[\"cols_selector\"][\"m2\"]] = temp_stock_market[m2_c[\"cols_selector\"][\"m2\"]].interpolate(method=\"linear\")\n",
        "\n",
        "  parameters[\"data_preparation\"][\"features\"] += [m2_c[\"cols_selector\"][\"m2\"]]\n",
        "  parameters[\"data_preparation\"][\"features\"] = list(set(parameters[\"data_preparation\"][\"features\"]))"
      ],
      "metadata": {
        "id": "Qja9ovrX0AX-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### VIX"
      ],
      "metadata": {
        "id": "l-UpWc74AuWZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if parameters[\"macro_economy_features\"][\"vix\"]:\n",
        "  vix_config = makro_config[\"macro_economy_features\"][\"vix\"]\n",
        "  VIX_PATH = os.path.join(GDRIVE_PATH,FEATURES_DATASET_PATH, \"VIX\")\n",
        "  vix = pd.read_excel(os.path.join(VIX_PATH, \"VIX Index.xlsx\"), **vix_config[\"read_data_config\"])\n",
        "  vix.columns = vix_config[\"cols_rename\"]\n",
        "\n",
        "  # Merge\n",
        "  temp_stock_market = temp_stock_market.merge(vix, on=vix_config[\"cols_selector\"][\"date\"], how=vix_config[\"preprocessing\"][\"merge\"])\n",
        "\n",
        "  if vix_config[\"preprocessing\"].get(\"fillna\"):\n",
        "    temp_stock_market[vix_config[\"cols_selector\"][\"value\"]] = temp_stock_market[vix_config[\"cols_selector\"][\"value\"]].interpolate(method=\"linear\")\n",
        "  \n",
        "  parameters[\"data_preparation\"][\"features\"] += vix_config[\"cols_selector\"][\"value\"]\n",
        "  parameters[\"data_preparation\"][\"features\"] = list(set(parameters[\"data_preparation\"][\"features\"]))"
      ],
      "metadata": {
        "id": "pI_KIsn3Ax0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Mikro"
      ],
      "metadata": {
        "id": "Ks7fpE5u3C_D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mikro_config = {\n",
        "    # \"usecols\": [\"Tanggal\", \"ROE\", \"NPM\", \"PER\", \"PBV\", \"Rasio Lancar\", \"DER\"],\n",
        "    # \"cols_rename\": [\"Date\", \"ROE\", \"NPM\", \"PER\", \"PBV\", \"Rasio Lancar\", \"DER\"],\n",
        "    \"usecols\": [\"Tanggal\", \"ROE\", \"NPM\", \"PER\"],\n",
        "    \"cols_rename\": [\"Date\", \"ROE\", \"NPM\", \"PER\"],\n",
        "    \"preprocessing\": {\n",
        "        \"merge\": \"left\",\n",
        "        \"fillna\": \"interpolate\" # After merge\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "vxgCBjSQ5fYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if parameters[\"micro_economy_features\"][\"ticker\"]:\n",
        "  MIKRO_EKONOMI_PATH = os.path.join(GDRIVE_PATH, FEATURES_DATASET_PATH, \"Micro Economi\")\n",
        "  MIKRO_EKONOMI_FILES = os.listdir(MIKRO_EKONOMI_PATH)\n",
        "  TICKER_LIST = [ticker.split(\".\")[0] for ticker in MIKRO_EKONOMI_FILES if ticker != \"Micro Ekonomi.xlsx\"]\n",
        "  MIKRO_EKONOMI_FILES = [os.path.join(MIKRO_EKONOMI_PATH, filename) for filename in MIKRO_EKONOMI_FILES]\n",
        "  MIKRO_EKONOMI_DICT = dict(zip(TICKER_LIST, MIKRO_EKONOMI_FILES))\n",
        "  for mikro_ekonomi_ticker, filepath in MIKRO_EKONOMI_DICT.items():\n",
        "    if re.match(parameters[\"micro_economy_features\"][\"ticker\"], mikro_ekonomi_ticker):\n",
        "      mikro_ekonomi = pd.read_excel(filepath, usecols=mikro_config[\"usecols\"])\n",
        "      mikro_ekonomi.columns = mikro_config[\"cols_rename\"]\n",
        "\n",
        "      temp_stock_market = temp_stock_market.merge(mikro_ekonomi, how=mikro_config[\"preprocessing\"][\"merge\"], on=\"Date\")\n",
        "      mikro_config[\"cols_rename\"].remove(\"Date\")\n",
        "\n",
        "      if mikro_config[\"preprocessing\"].get(\"fillna\"):\n",
        "        for col in mikro_config[\"cols_rename\"]:\n",
        "          temp_stock_market[col] = temp_stock_market[col].interpolate(method=\"linear\")\n",
        "\n",
        "      parameters[\"data_preparation\"][\"features\"] += mikro_config[\"cols_rename\"]\n",
        "      parameters[\"data_preparation\"][\"features\"] = list(set(parameters[\"data_preparation\"][\"features\"]))\n",
        "      break"
      ],
      "metadata": {
        "id": "MLSofBc53Ew8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### FINALIZE"
      ],
      "metadata": {
        "id": "T0Kc7qwKAwVO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Update date\n",
        "temp_stock_market.dropna(inplace=True)\n",
        "parameters[\"stock_market_detail\"][\"start_date\"] = str(np.min(temp_stock_market[\"Date\"].dt.date))\n",
        "parameters[\"stock_market_detail\"][\"end_date\"] = str(np.max(temp_stock_market[\"Date\"].dt.date))"
      ],
      "metadata": {
        "id": "xiGJZ5pd7A-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stock_market = temp_stock_market.set_index(\"Date\")"
      ],
      "metadata": {
        "id": "9WvXiRH56-FD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stock_market"
      ],
      "metadata": {
        "id": "cOFPVkSuVCMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing Data"
      ],
      "metadata": {
        "id": "lqfw3kpgyyoc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stock_market"
      ],
      "metadata": {
        "id": "wsbomnJ-WWmM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_stock_market = stock_market.copy()"
      ],
      "metadata": {
        "id": "lihlqVsA81pf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A. 1-2-Library Analysis Sentiment"
      ],
      "metadata": {
        "id": "mENwOBvIOZer"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install snscrape"
      ],
      "metadata": {
        "id": "BLVJcwKuyvIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LSkbbFtBc3XG"
      },
      "outputs": [],
      "source": [
        "import snscrape.modules.twitter as sntwitter\n",
        "import pandas as pd\n",
        "import random\n",
        "import pandas as pd\n",
        "import datetime"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A. 2-2- Scraping Data Twitter"
      ],
      "metadata": {
        "id": "Dho1fCOkOjxh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U32ssgm4y8u0"
      },
      "outputs": [],
      "source": [
        "#Don't Run Again!!!\n",
        "base = datetime.datetime.today()\n",
        "date_list = [base - datetime.timedelta(days=x) for x in range(6223)]\n",
        "date_list_strptime = [str(i.strftime(\"%Y-%m-%d\")) for i in date_list]\n",
        "datetime_range = date_list_strptime[14:]\n",
        "datetime_range"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QIPVaWr0c3XQ"
      },
      "outputs": [],
      "source": [
        "#Don't Run Again!!!\n",
        "datetimes = []\n",
        "for i in range(len(datetime_range)-1):\n",
        "    # print(datetime_range[i], datetime_range[i+1])\n",
        "    datetimes.append([datetime_range[i+1], datetime_range[i]])\n",
        "dates = datetimes[::-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tA9qnzKfc3XS"
      },
      "outputs": [],
      "source": [
        "#Don't Run Again!!!\n",
        "dates[::-1][5232:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GU4dlmH7c3XU"
      },
      "outputs": [],
      "source": [
        "#Don't Run Again!!!\n",
        "dates[-5][1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LO14Mm-Fc3XV"
      },
      "outputs": [],
      "source": [
        "#Don't Run Again!!!\n",
        "dp = pd.read_csv('data.csv')\n",
        "dp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3NEME2pc3XW"
      },
      "outputs": [],
      "source": [
        "#Don't Run Again!!!\n",
        "import snscrape.modules.twitter as sntwitter\n",
        "import pandas\n",
        "import csv\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Creating list to append tweet data to\n",
        "\n",
        "# Using TwitterSearchScraper to scrape data and append tweets to list\n",
        "for topic in ['resesi', 'perang', 'bahan bakar minyak']:\n",
        "    for date in dates[::-1]:\n",
        "        tweets_list2 = []\n",
        "        for i,tweet in enumerate(sntwitter.TwitterSearchScraper(f'{topic} since:{date[0]} until:{date[1]}').get_items()):\n",
        "            if i>0:\n",
        "                break\n",
        "            print(f'{topic}--{tweet.date}')\n",
        "            tweets_list2.append([tweet.date, tweet.id, tweet.content, tweet.user.username, tweet.url])\n",
        "            \n",
        "        # Creating a dataframe from the tweets list above\n",
        "        tweets_df2 = pd.DataFrame(tweets_list2, columns=['Datetime', 'Tweet Id', 'Text', 'Username', 'Url'])\n",
        "        data = pd.read_csv('tweets_per_keyword.csv')\n",
        "        datas = pd.concat([data, tweets_df2])\n",
        "        datas.to_csv('tweets_per_keyword.csv', index=False)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HsUVj49Dc3Xc"
      },
      "outputs": [],
      "source": [
        "#Don't Run Again!!!\n",
        "dates[::-1][5550:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RnJiV3Vcc3Xe"
      },
      "outputs": [],
      "source": [
        "#Don't Run Again!!!\n",
        "tweets_list2 = []\n",
        "for date in dates[::-1]:\n",
        "    for i,tweet in enumerate(sntwitter.TwitterSearchScraper(f'perang since:{date[0]} until:{date[1]}').get_items()):\n",
        "        if i>4:\n",
        "            break\n",
        "        tweets_list2.append(['perang',tweet.date, tweet.id, tweet.content, tweet.user.username, tweet.url])\n",
        "        print(f'{tweet.date}-{tweet.content}')\n",
        "        \n",
        "    # Creating a dataframe from the tweets list above\n",
        "tweets_df2 = pd.DataFrame(tweets_list2, columns=['keyword','Datetime', 'Tweet Id', 'Text', 'Username', 'Url'])\n",
        "# data = pd.read_csv('data_resesi.csv')\n",
        "# datas = pd.concat([data, tweets_df2])\n",
        "tweets_df2.to_csv(f'data_perang.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8MsfEF4ec3Xh"
      },
      "outputs": [],
      "source": [
        "#Don't Run Again!!!\n",
        "tweets_list3 = []\n",
        "for date in dates[::-1]:\n",
        "    for i,tweet in enumerate(sntwitter.TwitterSearchScraper(f'perang bahan bakar minyak resesi since:{date[0]} until:{date[1]}').get_items()):\n",
        "        if i>4:\n",
        "            break\n",
        "        tweets_list3.append(['perang, bahan bakar minyak, resesi',tweet.date, tweet.id, tweet.content, tweet.user.username, tweet.url])\n",
        "        print(f'{tweet.date}-{tweet.content}')\n",
        "        \n",
        "    # Creating a dataframe from the tweets list above\n",
        "tweets_df3 = pd.DataFrame(tweets_list3, columns=['keyword','Datetime', 'Tweet Id', 'Text', 'Username', 'Url'])\n",
        "# data = pd.read_csv('data_resesi.csv')\n",
        "# datas = pd.concat([data, tweets_df2])\n",
        "tweets_df3.to_csv(f'data_kumpulan_keyword.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A. 3-2-Text Preprocessing"
      ],
      "metadata": {
        "id": "HLfJrf69OV7z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Data"
      ],
      "metadata": {
        "id": "IoCLqRTRS74u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Data BBM"
      ],
      "metadata": {
        "id": "w0h_NBJ9Sh8F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "tweet_df_bbm = pd.read_csv(\"/content/drive/MyDrive/Datasets/Twitter/data_bbm.csv\")\n",
        "tweet_df_bbm.head(5)"
      ],
      "metadata": {
        "id": "m7250xNJSh8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Data Resesi"
      ],
      "metadata": {
        "id": "Geb4XO8CSh8D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "tweet_df_resesi = pd.read_csv(\"/content/drive/MyDrive/Datasets/Twitter/data_resesi.csv\")\n",
        "tweet_df_resesi.head(5)"
      ],
      "metadata": {
        "id": "kQBj6fkvSh8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Data Perang"
      ],
      "metadata": {
        "id": "9Ewn9Z63Sh8E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "tweet_df_perang = pd.read_csv(\"/content/drive/MyDrive/Datasets/Twitter/data_perang.csv\")\n",
        "tweet_df_perang.head(5)"
      ],
      "metadata": {
        "id": "Nj01s9nYSh8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing Data"
      ],
      "metadata": {
        "id": "BApPOqIzWt1D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Sastrawi"
      ],
      "metadata": {
        "id": "SmE25ExWXr5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tweepy\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "from networkx.readwrite import json_graph\n",
        "from plotly.offline import download_plotlyjs, init_notebook_mode,  iplot, plot\n",
        "init_notebook_mode(connected=True)\n",
        "\n",
        "# Machine Learning imports\n",
        "import nltk\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import joblib\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "import string\n",
        "import re\n",
        "from string import punctuation\n",
        "from collections import Counter"
      ],
      "metadata": {
        "id": "p846Mrj4W3CV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# helper function to clean tweets\n",
        "def processTweet(tweet):\n",
        "    # Remove HTML special entities (e.g. &amp;)\n",
        "    tweet = re.sub(r'\\&\\w*;', '', tweet)\n",
        "    #Convert @username to AT_USER\n",
        "    tweet = re.sub('@[^\\s]+','',tweet)\n",
        "    # Remove tickers\n",
        "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
        "    # To lowercase\n",
        "    tweet = tweet.lower()\n",
        "    # Remove hyperlinks\n",
        "    tweet = re.sub(r'https?:\\/\\/.*\\/\\w*', '', tweet)\n",
        "    # Remove hashtags\n",
        "    tweet = re.sub(r'#\\w*', '', tweet)\n",
        "    tweet = re.sub(r'bbm', '', tweet)\n",
        "    tweet = re.sub(r'bahan', '', tweet)\n",
        "    tweet = re.sub(r'bakar', '', tweet)\n",
        "    tweet = re.sub(r'minyak', '', tweet)\n",
        "    tweet = re.sub(r'resesi', '', tweet)\n",
        "    tweet = re.sub(r'perang', '', tweet)\n",
        "    # Remove Punctuation and split 's, 't, 've with a space for filter\n",
        "    tweet = re.sub(r'[' + punctuation.replace('@', '') + ']+', ' ', tweet)\n",
        "    # Remove words with 2 or fewer letters\n",
        "    tweet = re.sub(r'\\b\\w{1,2}\\b', '', tweet)\n",
        "    # Remove whitespace (including new line characters)\n",
        "    tweet = re.sub(r'\\s\\s+', ' ', tweet)\n",
        "    # Remove single space remaining at the front of the tweet.\n",
        "    tweet = tweet.lstrip(' ') \n",
        "    # Remove characters beyond Basic Multilingual Plane (BMP) of Unicode:\n",
        "    tweet = ''.join(c for c in tweet if c <= '\\uFFFF') \n",
        "    \n",
        "    return tweet\n",
        "\n",
        "tweet_df_bbm['Text']=tweet_df_bbm['Text'].astype(str)\n",
        "tweet_df_bbm = tweet_df_bbm.drop_duplicates('Text')\n",
        "tweet_df_resesi['Text']=tweet_df_bbm['Text'].astype(str)\n",
        "tweet_df_resesi = tweet_df_bbm.drop_duplicates('Text')\n",
        "tweet_df_perang['Text']=tweet_df_bbm['Text'].astype(str)\n",
        "tweet_df_perang = tweet_df_bbm.drop_duplicates('Text')\n",
        "# clean dataframe's text column\n",
        "tweet_df_bbm['Text_Clean'] = tweet_df_bbm['Text'].apply(processTweet)\n",
        "tweet_df_resesi['Text_Clean'] = tweet_df_resesi['Text'].apply(processTweet)\n",
        "tweet_df_perang['Text_Clean'] = tweet_df_perang['Text'].apply(processTweet)"
      ],
      "metadata": {
        "id": "qqM1dk8KXA5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_df_bbm.head(5)"
      ],
      "metadata": {
        "id": "IoYHyT1kYaqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_df_resesi.head(5)"
      ],
      "metadata": {
        "id": "42XhGomeYeTx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_df_perang.head(5)"
      ],
      "metadata": {
        "id": "Y6bWhU5FYeHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#stopwords baruu\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
        "list_stop_words = StopWordRemoverFactory().get_stop_words()\n",
        "stemming = StemmerFactory().create_stemmer()\n",
        "# tokenize helper function\n",
        "def text_process(raw_text):\n",
        "    # Check characters to see if they are in punctuation\n",
        "    nopunc = [char for char in list(raw_text) if char not in string.punctuation]\n",
        "\n",
        "    # Join the characters again to form the string.\n",
        "    nopunc = ''.join(nopunc)\n",
        "    \n",
        "    # Now just remove any stopwords\n",
        "    return [word for word in nopunc.lower().split() if word.lower() not in list_stop_words]\n",
        "\n",
        "# -------------------------------------------\n",
        "\n",
        "# tokenize message column and create a column for tokens\n",
        "tweet_df_bbm['tokens'] = tweet_df_bbm['Text_Clean'].apply(text_process)\n",
        "tweet_df_resesi['tokens'] = tweet_df_resesi['Text_Clean'].apply(text_process)\n",
        "tweet_df_perang['tokens'] = tweet_df_perang['Text_Clean'].apply(text_process)"
      ],
      "metadata": {
        "id": "KCJ8X6I2X8kJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_df_bbm.head(5)"
      ],
      "metadata": {
        "id": "E7ahUpM_YyHe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_df_resesi.head(5)"
      ],
      "metadata": {
        "id": "-03Xa-OCYzjt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_df_perang.head(5)"
      ],
      "metadata": {
        "id": "dNeIgz38Y01f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  A. 4-2-Sentiment Scoring using indonesian-roberta-base-sentiment Model and EDA"
      ],
      "metadata": {
        "id": "wk52l1ECOlLu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment Scoring"
      ],
      "metadata": {
        "id": "JwmQ5E0cZQzA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Don't Run Again!!!\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "wqwJ7JKxHk-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fbe3a77f-f2a4-49e2-8f1e-f1d4ffa4ffd0"
      },
      "outputs": [],
      "source": [
        "#Don't Run Again!!!\n",
        "from transformers import pipeline\n",
        "\n",
        "model = f\"w11wo/indonesian-roberta-base-sentiment-classifier\"\n",
        "\n",
        "sentiment_task = pipeline(\"sentiment-analysis\", model=model)\n",
        "sentiment_task(\"Covid cases are increasing fast!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Don't Run Again!!!\n",
        "from tqdm.notebook import tqdm"
      ],
      "metadata": {
        "id": "GIJRfjYvJOhd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### BBM"
      ],
      "metadata": {
        "id": "UdEoDU6RailB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "299f65e9-1925-40af-8418-b1b9eb4f759d"
      },
      "outputs": [],
      "source": [
        "#Don't Run Again!!!\n",
        "sent_results = {}\n",
        "count = 0\n",
        "for i, d in tqdm(tweet_df_bbm.iterrows(), total=len(tweet_df_bbm)):\n",
        "    sent = sentiment_task(d[\"Text\"])\n",
        "    sent_results[d[\"Tweet Id\"]] = sent\n",
        "    count += 1\n",
        "    # if count == 500:\n",
        "    #     break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c480516e-d531-4786-a21a-1d1f6b1c3687"
      },
      "outputs": [],
      "source": [
        "#Don't Run Again!!!\n",
        "sent_df = pd.DataFrame(sent_results).T\n",
        "sent_df[\"label\"] = sent_df[0].apply(lambda x: x[\"label\"])\n",
        "sent_df[\"score\"] = sent_df[0].apply(lambda x: x[\"score\"])\n",
        "sent_df = sent_df.merge(\n",
        "    tweet_df_bbm.set_index(\"Tweet Id\"), left_index=True, right_index=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Don't Run Again!!!\n",
        "sent_df"
      ],
      "metadata": {
        "id": "M3EMyAKaKFHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Don't Run Again!!!\n",
        "sent_df.label.value_counts()"
      ],
      "metadata": {
        "id": "2FacdANqh9mb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "216e2cee-45f4-468a-8501-c5386de7dd02"
      },
      "outputs": [],
      "source": [
        "#Don't Run Again!!!\n",
        "sent_df.groupby(\"label\")[\"score\"].plot(kind=\"hist\", bins=50)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ab7c904-09a4-4cc9-bb2d-98217f91e7ac"
      },
      "outputs": [],
      "source": [
        "#Don't Run Again!!!\n",
        "sent_df[\"score_\"] = sent_df[\"score\"]\n",
        "\n",
        "sent_df.loc[sent_df[\"label\"] == \"Negative\", \"score_\"] = (\n",
        "    sent_df.loc[sent_df[\"label\"] == \"Negative\"][\"score\"] * -1\n",
        ")\n",
        "\n",
        "sent_df.loc[sent_df[\"label\"] == \"Neutral\", \"score_\"] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29a587c4-da37-48c1-9064-2758d8eea31a"
      },
      "outputs": [],
      "source": [
        "#Don't Run Again!!!\n",
        "sent_df[\"score_\"].plot(kind=\"hist\", bins=50)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Don't Run Again!!!\n",
        "sent_df"
      ],
      "metadata": {
        "id": "ApKgaX1RLe30"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent_df = pd.read_csv('/content/drive/MyDrive/Datasets/Twitter/sent_df_bbm.csv')\n",
        "sent_df['Date'].isnull().sum()\n",
        "sent_df = sent_df.dropna(subset=['Date'])"
      ],
      "metadata": {
        "id": "f1pPc0BfPo1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67eac7fb-5cab-4ea5-ad18-b75f7e06adc7"
      },
      "outputs": [],
      "source": [
        "sent_df[\"Date\"] = pd.to_datetime(sent_df[\"Datetime\"], utc=True)\n",
        "sent_df[\"Date\"] = sent_df[\"Date\"].dt.date\n",
        "\n",
        "sent_daily = sent_df.groupby(\"Date\")[\"score_\"].mean()\n",
        "\n",
        "clx_df = preprocessed_stock_market\n",
        "clx_df = clx_df.reset_index()\n",
        "clx_df[\"Date\"] = clx_df[\"Date\"].dt.date\n",
        "clx_df = clx_df.set_index(\"Date\")\n",
        "\n",
        "sent_and_stock_bbm = sent_daily.to_frame(\"Sentiment_BBM\").merge(\n",
        "    clx_df, left_index=True, right_index=True\n",
        ")\n",
        "\n",
        "ax = sent_and_stock_bbm[\"Sentiment_BBM\"].plot(legend=\"Sentiment\")\n",
        "ax2 = ax.twinx()\n",
        "sent_and_stock_bbm[\"Close\"].plot(ax=ax2, color=\"orange\", legend=\"Closing Price\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sent_df"
      ],
      "metadata": {
        "id": "Edjou7i1YNaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent_and_stock_bbm"
      ],
      "metadata": {
        "id": "iD1lH2ujIRYl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Resesi"
      ],
      "metadata": {
        "id": "RZ0fLP68I0xn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BI14WbobI0xo"
      },
      "outputs": [],
      "source": [
        "#Don't Run Again!!!\n",
        "sent_results = {}\n",
        "count = 0\n",
        "for i, d in tqdm(tweet_df_resesi.iterrows(), total=len(tweet_df_resesi)):\n",
        "    sent = sentiment_task(d[\"Text\"])\n",
        "    sent_results[d[\"Tweet Id\"]] = sent\n",
        "    count += 1\n",
        "    # if count == 500:\n",
        "    #     break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6kQNraTYI0xp"
      },
      "outputs": [],
      "source": [
        "#Don't Run Again!!!\n",
        "sent_df = pd.DataFrame(sent_results).T\n",
        "sent_df[\"label\"] = sent_df[0].apply(lambda x: x[\"label\"])\n",
        "sent_df[\"score\"] = sent_df[0].apply(lambda x: x[\"score\"])\n",
        "sent_df = sent_df.merge(\n",
        "    tweet_df_bbm.set_index(\"Tweet Id\"), left_index=True, right_index=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Don't Run Again!!!\n",
        "sent_df"
      ],
      "metadata": {
        "id": "vJYZVd04I0xq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Don't Run Again!!!\n",
        "sent_df.label.value_counts()"
      ],
      "metadata": {
        "id": "Kc4hCki_iXxq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WkC3Mlx8I0xq"
      },
      "outputs": [],
      "source": [
        "#Don't Run Again!!!\n",
        "sent_df.groupby(\"label\")[\"score\"].plot(kind=\"hist\", bins=50)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bcv-gFdDI0xr"
      },
      "outputs": [],
      "source": [
        "#Don't Run Again!!!\n",
        "sent_df[\"score_\"] = sent_df[\"score\"]\n",
        "\n",
        "sent_df.loc[sent_df[\"label\"] == \"Negative\", \"score_\"] = (\n",
        "    sent_df.loc[sent_df[\"label\"] == \"Negative\"][\"score\"] * -1\n",
        ")\n",
        "\n",
        "sent_df.loc[sent_df[\"label\"] == \"Neutral\", \"score_\"] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1b7Q2uRXI0xr"
      },
      "outputs": [],
      "source": [
        "#Don't Run Again!!!\n",
        "sent_df[\"score_\"].plot(kind=\"hist\", bins=50)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Don't Run Again!!!\n",
        "sent_df"
      ],
      "metadata": {
        "id": "XbTHzhqjI0xr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent_df = pd.read_csv('/content/drive/MyDrive/Datasets/Twitter/sent_df_resesi.csv')\n",
        "sent_df['Date'].isnull().sum()\n",
        "sent_df = sent_df.dropna(subset=['Date'])"
      ],
      "metadata": {
        "id": "fWwZGsB0Pg_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tTZ8qyLPI0xs"
      },
      "outputs": [],
      "source": [
        "sent_df[\"Date\"] = pd.to_datetime(sent_df[\"Datetime\"], utc=True)\n",
        "sent_df[\"Date\"] = sent_df[\"Date\"].dt.date\n",
        "\n",
        "sent_daily = sent_df.groupby(\"Date\")[\"score_\"].mean()\n",
        "\n",
        "clx_df = sent_and_stock_bbm\n",
        "clx_df = clx_df.reset_index()\n",
        "# clx_df[\"Date\"] = clx_df[\"Date\"].dt.date\n",
        "clx_df = clx_df.set_index(\"Date\")\n",
        "\n",
        "sent_and_stock_resesi = sent_daily.to_frame(\"Sentiment_Resesi\").merge(\n",
        "    clx_df, left_index=True, right_index=True\n",
        ")\n",
        "\n",
        "ax = sent_and_stock_resesi[\"Sentiment_Resesi\"].plot(legend=\"Sentiment\")\n",
        "ax2 = ax.twinx()\n",
        "sent_and_stock_resesi[\"Close\"].plot(ax=ax2, color=\"orange\", legend=\"Closing Price\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sent_df"
      ],
      "metadata": {
        "id": "gCDJGIvFYj74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent_and_stock_resesi"
      ],
      "metadata": {
        "id": "W9z5ROoJI0xs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Perang"
      ],
      "metadata": {
        "id": "VlKBLf2xI1mf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-lu1i-ecI1mf"
      },
      "outputs": [],
      "source": [
        "#Don't Run Again!!!\n",
        "sent_results = {}\n",
        "count = 0\n",
        "for i, d in tqdm(tweet_df_perang.iterrows(), total=len(tweet_df_perang)):\n",
        "    sent = sentiment_task(d[\"Text\"])\n",
        "    sent_results[d[\"Tweet Id\"]] = sent\n",
        "    count += 1\n",
        "    # if count == 500:\n",
        "    #     break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tpUP3eipI1mg"
      },
      "outputs": [],
      "source": [
        "#Don't Run Again!!!\n",
        "sent_df = pd.DataFrame(sent_results).T\n",
        "sent_df[\"label\"] = sent_df[0].apply(lambda x: x[\"label\"])\n",
        "sent_df[\"score\"] = sent_df[0].apply(lambda x: x[\"score\"])\n",
        "sent_df = sent_df.merge(\n",
        "    tweet_df_bbm.set_index(\"Tweet Id\"), left_index=True, right_index=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Don't Run Again!!!\n",
        "sent_df"
      ],
      "metadata": {
        "id": "2EnMO9QnI1mg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Don't Run Again!!!\n",
        "sent_df.label.value_counts()"
      ],
      "metadata": {
        "id": "p6HGpuDziZV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lBs60ctDI1mg"
      },
      "outputs": [],
      "source": [
        "#Don't Run Again!!!\n",
        "sent_df.groupby(\"label\")[\"score\"].plot(kind=\"hist\", bins=50)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hpoyyj4KI1mh"
      },
      "outputs": [],
      "source": [
        "#Don't Run Again!!!\n",
        "sent_df[\"score_\"] = sent_df[\"score\"]\n",
        "\n",
        "sent_df.loc[sent_df[\"label\"] == \"Negative\", \"score_\"] = (\n",
        "    sent_df.loc[sent_df[\"label\"] == \"Negative\"][\"score\"] * -1\n",
        ")\n",
        "\n",
        "sent_df.loc[sent_df[\"label\"] == \"Neutral\", \"score_\"] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1X_ecA8I1mh"
      },
      "outputs": [],
      "source": [
        "#Don't Run Again!!!\n",
        "sent_df[\"score_\"].plot(kind=\"hist\", bins=50)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Don't Run Again!!!\n",
        "sent_df"
      ],
      "metadata": {
        "id": "_hMqZh6SI1mh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent_df = pd.read_csv('/content/drive/MyDrive/Datasets/Twitter/sent_df_perang.csv')\n",
        "sent_df['Date'].isnull().sum()\n",
        "sent_df = sent_df.dropna(subset=['Date'])"
      ],
      "metadata": {
        "id": "QgIjQQRCO8vT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ec9amm6nI1mh"
      },
      "outputs": [],
      "source": [
        "sent_df[\"Date\"] = pd.to_datetime(sent_df[\"Datetime\"], utc=True)\n",
        "sent_df[\"Date\"] = sent_df[\"Date\"].dt.date\n",
        "\n",
        "sent_daily = sent_df.groupby(\"Date\")[\"score_\"].mean()\n",
        "\n",
        "clx_df = sent_and_stock_resesi\n",
        "clx_df = clx_df.reset_index()\n",
        "# clx_df[\"Date\"] = clx_df[\"Date\"].dt.date\n",
        "clx_df = clx_df.set_index(\"Date\")\n",
        "\n",
        "sent_and_stock_perang = sent_daily.to_frame(\"Sentiment_Perang\").merge(\n",
        "    clx_df, left_index=True, right_index=True\n",
        ")\n",
        "\n",
        "ax = sent_and_stock_perang[\"Sentiment_Perang\"].plot(legend=\"Sentiment\")\n",
        "ax2 = ax.twinx()\n",
        "sent_and_stock_perang[\"Close\"].plot(ax=ax2, color=\"orange\", legend=\"Closing Price\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sent_df"
      ],
      "metadata": {
        "id": "7V8GhXRKYqhm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent_and_stock_perang"
      ],
      "metadata": {
        "id": "JkJk35exI1mi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent_and_stock_perang.columns"
      ],
      "metadata": {
        "id": "iKs4RiNDZ52M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent_and_stock_perang = sent_and_stock_perang [['Open', 'High',\n",
        "       'Low', 'Close', 'Inflasi', 'Kurs', 'M2', 'Vix Open', 'Vix High',\n",
        "       'Vix Low', 'Vix Close', 'ROE', 'NPM', 'PER', 'Sentiment_Perang', 'Sentiment_Resesi', 'Sentiment_BBM',]]"
      ],
      "metadata": {
        "id": "Fn2wEK2SLzoy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent_and_stock_perang.columns"
      ],
      "metadata": {
        "id": "vvT0A-OWL2cP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent_and_stock_perang.head(5)"
      ],
      "metadata": {
        "id": "xtXl6CkYL5fa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Don't Run Again!!!\n",
        "sent_and_stock_perang.to_csv(\"/content/drive/MyDrive/Datasets/New Project/Dataset/final_dataset.csv\")"
      ],
      "metadata": {
        "id": "d8xxY5VoaWmC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ls-JaqDEP-Lx"
      },
      "source": [
        "### EDA using Word Cloud"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "tweet_df_bbm1 = pd.read_csv(\"/content/drive/MyDrive/Datasets/Twitter/sent_df_bbm.csv\")\n",
        "tweet_df_resesi1 = pd.read_csv(\"/content/drive/MyDrive/Datasets/Twitter/sent_df_resesi.csv\")\n",
        "tweet_df_perang1= pd.read_csv(\"/content/drive/MyDrive/Datasets/Twitter/sent_df_perang.csv\")"
      ],
      "metadata": {
        "id": "8tazwgGpeFOX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_df_perang1['tokens'] = tweet_df_perang['tokens']\n",
        "tweet_df_bbm1['tokens'] = tweet_df_bbm['tokens']\n",
        "tweet_df_resesi1['tokens'] = tweet_df_resesi['tokens']"
      ],
      "metadata": {
        "id": "6MajOCys92Yz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_df_bbm = tweet_df_bbm1.dropna(subset=['tokens'])\n",
        "tweet_df_resesi = tweet_df_resesi1.dropna(subset=['tokens'])\n",
        "tweet_df_perang = tweet_df_perang1.dropna(subset=['tokens'])"
      ],
      "metadata": {
        "id": "Y0krC1XFzwu4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jSu5ujjXNfS2"
      },
      "outputs": [],
      "source": [
        "df2a= tweet_df_bbm[tweet_df_bbm['label']=='positive']\n",
        "df2b= tweet_df_resesi[tweet_df_resesi['label']=='positive']\n",
        "df2c= tweet_df_perang[tweet_df_perang['label']=='positive']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dGNnNycDzzK_"
      },
      "outputs": [],
      "source": [
        "df1a= tweet_df_bbm[tweet_df_bbm['label']=='neutral']\n",
        "df1b= tweet_df_resesi[tweet_df_resesi['label']=='neutral']\n",
        "df1c= tweet_df_perang[tweet_df_perang['label']=='neutral']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AnjKwOgo0Lpv"
      },
      "outputs": [],
      "source": [
        "df0a= tweet_df_bbm[tweet_df_bbm['label']=='negative']\n",
        "df0b= tweet_df_resesi[tweet_df_resesi['label']=='negative']\n",
        "df0c= tweet_df_perang[tweet_df_perang['label']=='negative']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Pie Chart Sentiment"
      ],
      "metadata": {
        "id": "xptr8Xj6RJ1m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### BBM"
      ],
      "metadata": {
        "id": "L3IP0kDdSHYn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_df_bbm.label.value_counts()"
      ],
      "metadata": {
        "id": "0VsPNRDIRQFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_df_bbm = tweet_df_bbm.loc[tweet_df_bbm['label']!='https://twitter.com/ZhafirahUmay/status/598518953749520384']"
      ],
      "metadata": {
        "id": "gYfTm6rETAFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_size = plt.rcParams[\"figure.figsize\"] \n",
        "print(plot_size[0]) \n",
        "print(plot_size[1])\n",
        "\n",
        "plot_size[0] = 8\n",
        "plot_size[1] = 6\n",
        "plt.rcParams[\"figure.figsize\"] = plot_size \n",
        "\n",
        "tweet_df_bbm.groupby('label').count()['Text'].plot(kind='pie', labels= ('negative', 'neutral', 'positive'), autopct='%1.0f%%')"
      ],
      "metadata": {
        "id": "G3qoeXjDSOr2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Resesi"
      ],
      "metadata": {
        "id": "vEJOjRLyTfHw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_df_resesi.label.value_counts()"
      ],
      "metadata": {
        "id": "KRB0VG_iTfHx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_df_resesi = tweet_df_resesi.loc[tweet_df_resesi['label']!='https://twitter.com/ZhafirahUmay/status/598518953749520384']"
      ],
      "metadata": {
        "id": "SCliJisNTfHx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_size = plt.rcParams[\"figure.figsize\"] \n",
        "print(plot_size[0]) \n",
        "print(plot_size[1])\n",
        "\n",
        "plot_size[0] = 8\n",
        "plot_size[1] = 6\n",
        "plt.rcParams[\"figure.figsize\"] = plot_size \n",
        "\n",
        "tweet_df_resesi.groupby('label').count()['Text'].plot(kind='pie', labels= ('negative', 'neutral', 'positive'), autopct='%1.0f%%')"
      ],
      "metadata": {
        "id": "PjT1wY7QTfHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Perang"
      ],
      "metadata": {
        "id": "JVS3MDYuTfg_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_df_perang.label.value_counts()"
      ],
      "metadata": {
        "id": "Lf27Z5OOTfg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_df_perang = tweet_df_perang.loc[tweet_df_perang['label']!='https://twitter.com/ZhafirahUmay/status/598518953749520384']"
      ],
      "metadata": {
        "id": "0tf8xh8mTfg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_size = plt.rcParams[\"figure.figsize\"] \n",
        "print(plot_size[0]) \n",
        "print(plot_size[1])\n",
        "\n",
        "plot_size[0] = 8\n",
        "plot_size[1] = 6\n",
        "plt.rcParams[\"figure.figsize\"] = plot_size \n",
        "\n",
        "tweet_df_perang.groupby('label').count()['Text'].plot(kind='pie', labels= ('negative', 'neutral', 'positive'), autopct='%1.0f%%')"
      ],
      "metadata": {
        "id": "3obroGvCTfhA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_W6tso90Qy3"
      },
      "source": [
        "#### WC Total"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter"
      ],
      "metadata": {
        "id": "tX57uaji0ot5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### BBM"
      ],
      "metadata": {
        "id": "cLW4KWoJBS4G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DKp1YQTGP-L0"
      },
      "outputs": [],
      "source": [
        "all_words = []\n",
        "for line in tweet_df_bbm['tokens']: \n",
        "    all_words.extend(line)  \n",
        "# create a word frequency dictionary\n",
        "wordfreq = Counter(all_words)\n",
        "wordfreq.most_common(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D_WqJA4HP-L8"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "wordcloud = WordCloud(width=900,\n",
        "                      height=500,\n",
        "                      max_words=500,\n",
        "                      max_font_size=100,\n",
        "                      relative_scaling=0.5,\n",
        "                      colormap='gist_rainbow',\n",
        "                      normalize_plurals=True).generate_from_frequencies(wordfreq)\n",
        "plt.figure(figsize=(17,14))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Resesi"
      ],
      "metadata": {
        "id": "J7zLjFGuByub"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_words = []\n",
        "for line in tweet_df_resesi['tokens']: \n",
        "    all_words.extend(line)  \n",
        "# create a word frequency dictionary\n",
        "wordfreq = Counter(all_words)\n",
        "wordfreq.most_common(10)"
      ],
      "metadata": {
        "id": "d5l2Vdi7j3Fd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "wordcloud = WordCloud(width=900,\n",
        "                      height=500,\n",
        "                      max_words=500,\n",
        "                      max_font_size=100,\n",
        "                      relative_scaling=0.5,\n",
        "                      colormap='gist_rainbow',\n",
        "                      normalize_plurals=True).generate_from_frequencies(wordfreq)\n",
        "plt.figure(figsize=(17,14))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3lzPJyxKj4lB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Perang"
      ],
      "metadata": {
        "id": "17srCnMKBwUe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_words = []\n",
        "for line in tweet_df_perang['tokens']: \n",
        "    all_words.extend(line)  \n",
        "# create a word frequency dictionary\n",
        "wordfreq = Counter(all_words)\n",
        "wordfreq.most_common(10)"
      ],
      "metadata": {
        "id": "FJA1kT7ej3im"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "wordcloud = WordCloud(width=900,\n",
        "                      height=500,\n",
        "                      max_words=500,\n",
        "                      max_font_size=100,\n",
        "                      relative_scaling=0.5,\n",
        "                      colormap='gist_rainbow',\n",
        "                      normalize_plurals=True).generate_from_frequencies(wordfreq)\n",
        "plt.figure(figsize=(17,14))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "u0G-_V8Sj5R1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ne8QT1ONqdv"
      },
      "source": [
        "#### WC Positive"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### BBM"
      ],
      "metadata": {
        "id": "VlS-d15hBYK4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-CEMo0yNqdv"
      },
      "outputs": [],
      "source": [
        "all_words = []\n",
        "for line in df2a['tokens']: \n",
        "    all_words.extend(line)  \n",
        "# create a word frequency dictionary\n",
        "wordfreq1 = Counter(all_words)\n",
        "wordfreq1.most_common(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v49ym84gNqdw"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "wordcloud = WordCloud(width=900,\n",
        "                      height=500,\n",
        "                      max_words=500,\n",
        "                      max_font_size=100,\n",
        "                      relative_scaling=0.5,\n",
        "                      colormap='gist_rainbow',\n",
        "                      normalize_plurals=True).generate_from_frequencies(wordfreq1)\n",
        "plt.figure(figsize=(17,14))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Resesi"
      ],
      "metadata": {
        "id": "YsgxR8-FBt2r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_words = []\n",
        "for line in df2b['tokens']: \n",
        "    all_words.extend(line)  \n",
        "# create a word frequency dictionary\n",
        "wordfreq1 = Counter(all_words)\n",
        "wordfreq1.most_common(10)"
      ],
      "metadata": {
        "id": "BStDvT0Bk0x3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "wordcloud = WordCloud(width=900,\n",
        "                      height=500,\n",
        "                      max_words=500,\n",
        "                      max_font_size=100,\n",
        "                      relative_scaling=0.5,\n",
        "                      colormap='gist_rainbow',\n",
        "                      normalize_plurals=True).generate_from_frequencies(wordfreq1)\n",
        "plt.figure(figsize=(17,14))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3vLgtuLkk3p4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Perang"
      ],
      "metadata": {
        "id": "rlA4XgfpBrDy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_words = []\n",
        "for line in df2c['tokens']: \n",
        "    all_words.extend(line)  \n",
        "# create a word frequency dictionary\n",
        "wordfreq1 = Counter(all_words)\n",
        "wordfreq1.most_common(10)"
      ],
      "metadata": {
        "id": "pwmpYZBqk1Sb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "wordcloud = WordCloud(width=900,\n",
        "                      height=500,\n",
        "                      max_words=500,\n",
        "                      max_font_size=100,\n",
        "                      relative_scaling=0.5,\n",
        "                      colormap='gist_rainbow',\n",
        "                      normalize_plurals=True).generate_from_frequencies(wordfreq1)\n",
        "plt.figure(figsize=(17,14))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "W4SZl2sHk5GL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mVDvGGz0WiB"
      },
      "source": [
        "#### WC Neutral"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### BBM"
      ],
      "metadata": {
        "id": "tyfT-129BaiN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aGUzQHm60UFa"
      },
      "outputs": [],
      "source": [
        "all_words = []\n",
        "for line in df1a['tokens']: \n",
        "    all_words.extend(line)  \n",
        "# create a word frequency dictionary\n",
        "wordfreq1 = Counter(all_words)\n",
        "wordfreq1.most_common(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6xMD_guU0kMW"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "wordcloud = WordCloud(width=900,\n",
        "                      height=500,\n",
        "                      max_words=500,\n",
        "                      max_font_size=100,\n",
        "                      relative_scaling=0.5,\n",
        "                      colormap='gist_rainbow',\n",
        "                      normalize_plurals=True).generate_from_frequencies(wordfreq1)\n",
        "plt.figure(figsize=(17,14))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Resesi"
      ],
      "metadata": {
        "id": "BuFQHv9lBn_Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_words = []\n",
        "for line in df1b['tokens']: \n",
        "    all_words.extend(line)  \n",
        "# create a word frequency dictionary\n",
        "wordfreq1 = Counter(all_words)\n",
        "wordfreq1.most_common(10)"
      ],
      "metadata": {
        "id": "0_B-Cgo9k7u2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "wordcloud = WordCloud(width=900,\n",
        "                      height=500,\n",
        "                      max_words=500,\n",
        "                      max_font_size=100,\n",
        "                      relative_scaling=0.5,\n",
        "                      colormap='gist_rainbow',\n",
        "                      normalize_plurals=True).generate_from_frequencies(wordfreq1)\n",
        "plt.figure(figsize=(17,14))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jyv90R0Xk_fz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Perang"
      ],
      "metadata": {
        "id": "1TN_E1WSBl2j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_words = []\n",
        "for line in df1c['tokens']: \n",
        "    all_words.extend(line)  \n",
        "# create a word frequency dictionary\n",
        "wordfreq1 = Counter(all_words)\n",
        "wordfreq1.most_common(10)"
      ],
      "metadata": {
        "id": "iGkVoDQhk9Fk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "wordcloud = WordCloud(width=900,\n",
        "                      height=500,\n",
        "                      max_words=500,\n",
        "                      max_font_size=100,\n",
        "                      relative_scaling=0.5,\n",
        "                      colormap='gist_rainbow',\n",
        "                      normalize_plurals=True).generate_from_frequencies(wordfreq1)\n",
        "plt.figure(figsize=(17,14))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MU9kXLeHlA9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0AGKN_z0Y6V"
      },
      "source": [
        "#### WC Negative"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### BBM"
      ],
      "metadata": {
        "id": "YXn0mHKzBc7F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qCf3s95F0bSX"
      },
      "outputs": [],
      "source": [
        "all_words = []\n",
        "for line in df0a['tokens']: \n",
        "    all_words.extend(line)  \n",
        "# create a word frequency dictionary\n",
        "wordfreq0 = Counter(all_words)\n",
        "wordfreq0.most_common(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4TlHO05t0qPW"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "wordcloud = WordCloud(width=900,\n",
        "                      height=500,\n",
        "                      max_words=500,\n",
        "                      max_font_size=100,\n",
        "                      relative_scaling=0.5,\n",
        "                      colormap='gist_rainbow',\n",
        "                      normalize_plurals=True).generate_from_frequencies(wordfreq0)\n",
        "plt.figure(figsize=(17,14))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Resesi"
      ],
      "metadata": {
        "id": "ck6va0-FBfeJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_words = []\n",
        "for line in df0b['tokens']: \n",
        "    all_words.extend(line)  \n",
        "# create a word frequency dictionary\n",
        "wordfreq0 = Counter(all_words)\n",
        "wordfreq0.most_common(10)"
      ],
      "metadata": {
        "id": "5g5zwLi0MnXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "wordcloud = WordCloud(width=900,\n",
        "                      height=500,\n",
        "                      max_words=500,\n",
        "                      max_font_size=100,\n",
        "                      relative_scaling=0.5,\n",
        "                      colormap='gist_rainbow',\n",
        "                      normalize_plurals=True).generate_from_frequencies(wordfreq0)\n",
        "plt.figure(figsize=(17,14))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AU-HbEpflLZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Perang"
      ],
      "metadata": {
        "id": "pSBt57yNBiIw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_words = []\n",
        "for line in df0c['tokens']: \n",
        "    all_words.extend(line)  \n",
        "# create a word frequency dictionary\n",
        "wordfreq0 = Counter(all_words)\n",
        "wordfreq0.most_common(10)"
      ],
      "metadata": {
        "id": "jU7HAC_nlEQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "wordcloud = WordCloud(width=900,\n",
        "                      height=500,\n",
        "                      max_words=500,\n",
        "                      max_font_size=100,\n",
        "                      relative_scaling=0.5,\n",
        "                      colormap='gist_rainbow',\n",
        "                      normalize_plurals=True).generate_from_frequencies(wordfreq0)\n",
        "plt.figure(figsize=(17,14))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Z-auSOXflMR7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}